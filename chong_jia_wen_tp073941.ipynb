{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheongyeechian/DLI/blob/main/chong_jia_wen_tp073941.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i47nk1rc3yxp"
      },
      "source": [
        "# LBP Hyperfast Hybrid (with τ-sweep & optional class-weight sweep)\n",
        "\n",
        "**Goal:** Beat the paper's metrics (Acc≈94.1, Prec≈94.12, Rec≈95.35, F1≈94.73) in under ~5 minutes on CPU.\n",
        "\n",
        "**Pipeline:**\n",
        "1) **Stage A** — Fast Linear Prior: Character n-gram `HashingVectorizer` + `SGDClassifier (log_loss)` + Platt scaling.\n",
        "2) **Stage B** — LBP-lite on **uncertain** subset only: Compact URL↔Domain↔TokenBucket graph with min-sum BP for a few iterations.\n",
        "\n",
        "**New in this notebook:**\n",
        "- Prints **training time** (vectorization, model train, calibrator) and **testing time** (Stage A prediction, graph build, LBP, stitching, total).\n",
        "- **τ-sweep** on a small validation split to auto-pick the best decision threshold for LBP cost ratio.\n",
        "- **Optional** lightweight **class-weight sweep** for the positive class (phishing). Disabled by default to meet time budget; enable in `CONFIG` if needed.\n",
        "\n",
        "**Expected files:**\n",
        "- `/mnt/data/Train_data.csv` (1.2M rows)\n",
        "- `/mnt/data/Test_data.csv`  (361k rows)\n",
        "\n",
        "Columns expected: `url`, `label` where label is `good`/`bad` or `0`/`1`."
      ],
      "id": "i47nk1rc3yxp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCjLmcqo3yxs",
        "outputId": "77e8dc70-7fb3-47e6-b994-46e4d761844c"
      },
      "source": [
        "# ==========================\n",
        "# Config\n",
        "# ==========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CONFIG = {\n",
        "    # Data paths\n",
        "    \"train_csv\": \"/content/drive/My Drive/Colab Notebooks/Mendeley/Train_data.csv\",\n",
        "    \"test_csv\":  \"/content/drive/My Drive/Colab Notebooks/Mendeley/Test_data.csv\",\n",
        "\n",
        "    # Vectorizer & model\n",
        "    \"char_ngram_range\": (3,5),\n",
        "    \"n_features\": 2**20,     # try 2**18 for faster/smaller\n",
        "    \"sgd_alpha\": 1e-5,       # regularization\n",
        "    \"random_state\": 42,\n",
        "\n",
        "    # Splits\n",
        "    \"valid_size\": 0.05,      # validation from TRAIN for tau/class-weight tuning\n",
        "    \"calib_size\": 0.03,      # small calibration split from the remaining train\n",
        "\n",
        "    # Uncertain window for handing off to LBP-lite\n",
        "    \"uncertain_low\": 0.40,\n",
        "    \"uncertain_high\": 0.60,\n",
        "\n",
        "    # LBP-lite\n",
        "    \"lbp_iters\": 6,\n",
        "    \"ths_plus\": 0.6,\n",
        "    \"ths_minus\": 1.0,\n",
        "\n",
        "    # Tau sweep (validation only); best tau is used on TEST\n",
        "    \"enable_tau_sweep\": True,\n",
        "    \"tau_grid\": [round(x,2) for x in [0.45 + 0.01*i for i in range(11)]],\n",
        "\n",
        "    # Optional class-weight sweep for phishing class (positive class=1)\n",
        "    \"enable_class_weight_sweep\": False,  # set True to enable (slower)\n",
        "    \"pos_weight_grid\": [1.0, 1.5],      # keep small to preserve time budget\n",
        "}\n",
        "CONFIG"
      ],
      "id": "WCjLmcqo3yxs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_csv': '/content/drive/My Drive/Colab Notebooks/Mendeley/Train_data.csv',\n",
              " 'test_csv': '/content/drive/My Drive/Colab Notebooks/Mendeley/Test_data.csv',\n",
              " 'char_ngram_range': (3, 5),\n",
              " 'n_features': 1048576,\n",
              " 'sgd_alpha': 1e-05,\n",
              " 'random_state': 42,\n",
              " 'valid_size': 0.05,\n",
              " 'calib_size': 0.03,\n",
              " 'uncertain_low': 0.4,\n",
              " 'uncertain_high': 0.6,\n",
              " 'lbp_iters': 6,\n",
              " 'ths_plus': 0.6,\n",
              " 'ths_minus': 1.0,\n",
              " 'enable_tau_sweep': True,\n",
              " 'tau_grid': [0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55],\n",
              " 'enable_class_weight_sweep': False,\n",
              " 'pos_weight_grid': [1.0, 1.5]}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qYoFlbg3yxt"
      },
      "source": [
        "# ==========================\n",
        "# Imports & Utilities\n",
        "# ==========================\n",
        "import pandas as pd, numpy as np, re\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "RS = CONFIG[\"random_state\"]\n",
        "\n",
        "def norm_url(u: str) -> str:\n",
        "    if not isinstance(u, str):\n",
        "        u = str(u)\n",
        "    u = u.strip().lower()\n",
        "    return re.sub(r'^https?://', '', u)\n",
        "\n",
        "def get_domain(u: str) -> str:\n",
        "    return u.split('/')[0]\n",
        "\n",
        "def shingles(s, k=5):\n",
        "    if not s: return set()\n",
        "    L = len(s)\n",
        "    return { s[i:i+k] for i in range(max(0, L-k+1)) }\n",
        "\n",
        "def bucket_id(s):\n",
        "    # simple LSH-like bucket by XOR hashing a few shingles\n",
        "    xs = list(shingles(s, 5))\n",
        "    if not xs: return 0\n",
        "    h = 0\n",
        "    for w in xs[:64]:\n",
        "        h ^= hash(w)\n",
        "    return h & ((1<<20)-1)\n",
        "\n",
        "def ensure_labels(series):\n",
        "    s = series.copy()\n",
        "    if s.dtype == 'O':\n",
        "        s = s.str.lower().map({'good':0,'bad':1})\n",
        "    s = s.fillna(0).astype(int)\n",
        "    return s.values\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "def print_metrics(tag, y_true, y_pred):\n",
        "    acc, prec, rec, f1 = metrics(y_true, y_pred)\n",
        "    print(f\"{tag:20s} Acc={acc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\")\n",
        "    return acc,prec,rec,f1"
      ],
      "id": "0qYoFlbg3yxt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUMDEc623yxt",
        "outputId": "1db302a2-b880-44b3-9cdd-a272f45283b9"
      },
      "source": [
        "# ==========================\n",
        "# Load & Split\n",
        "# ==========================\n",
        "train_full = pd.read_csv(CONFIG['train_csv'])\n",
        "test = pd.read_csv(CONFIG['test_csv'])\n",
        "for df in (train_full, test):\n",
        "    df['u'] = df['url'].map(norm_url)\n",
        "y_full = ensure_labels(train_full['label'])\n",
        "y_test = ensure_labels(test['label'])\n",
        "\n",
        "# Validation split from TRAIN for tau/class-weight tuning\n",
        "train_rest, valid = train_test_split(train_full, test_size=CONFIG['valid_size'], stratify=y_full, random_state=RS)\n",
        "y_rest  = ensure_labels(train_rest['label'])\n",
        "y_valid = ensure_labels(valid['label'])\n",
        "print(f\"Split: train_rest={len(train_rest)}, valid={len(valid)}\")"
      ],
      "id": "QUMDEc623yxt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data in 8.72s | train=1200000 test=361934\n",
            "Split: train_rest=1140000, valid=60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swp1Kwmq3yxu"
      },
      "source": [
        "# ==========================\n",
        "# Stage A training (with timing) — returns vectorizer, clf, calibrator\n",
        "# ==========================\n",
        "def train_stageA(train_df, y, pos_weight=1.0):\n",
        "    cfg = CONFIG\n",
        "    # Vectorize\n",
        "    vec = HashingVectorizer(analyzer='char', ngram_range=cfg['char_ngram_range'],\n",
        "                            n_features=cfg['n_features'], norm='l2', alternate_sign=False)\n",
        "    X = vec.transform(train_df['u'])\n",
        "\n",
        "    # Split for calibrator\n",
        "    X_tr, X_cal, y_tr, y_cal = train_test_split(X, y, test_size=cfg['calib_size'], stratify=y, random_state=RS)\n",
        "\n",
        "    # Model (class_weight either 'balanced' or manual)\n",
        "    cw = {0:1.0, 1:pos_weight} if pos_weight!= 'balanced' else 'balanced'\n",
        "    clf = SGDClassifier(loss='log_loss', alpha=cfg['sgd_alpha'], class_weight=cw,\n",
        "                        early_stopping=True, n_iter_no_change=3, validation_fraction=0.02, random_state=RS)\n",
        "\n",
        "    # Platt scaling\n",
        "    scores_cal = clf.decision_function(X_cal).reshape(-1,1)\n",
        "    cal = LogisticRegression(max_iter=100, random_state=RS)\n",
        "\n",
        "    return vec, clf, cal\n",
        "\n",
        "def predict_proba_stageA(vec, clf, cal, df):\n",
        "    X = vec.transform(df['u'])\n",
        "    s = clf.decision_function(X).reshape(-1,1)\n",
        "    p1 = cal.predict_proba(s)[:,1]\n",
        "    P = np.c_[1-p1, p1]\n",
        "    return P"
      ],
      "id": "Swp1Kwmq3yxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhRK79H73yxu"
      },
      "source": [
        "# ==========================\n",
        "# LBP-lite helpers (graph build + inference)\n",
        "# ==========================\n",
        "def build_graph_for(df_unknown, df_observed, P_unknown):\n",
        "    \"\"\"Build compact graph with unknown URL nodes from df_unknown and observed neighbors from df_observed.\n",
        "       Returns (prior, neighbors, edges, sim_cache, idx_maps) where unknown nodes are [0..nu-1].\"\"\"\n",
        "    cfg = CONFIG\n",
        "    ths_plus, ths_minus = cfg['ths_plus'], cfg['ths_minus']\n",
        "\n",
        "    # Unknown set\n",
        "    nu = len(df_unknown)\n",
        "    U_dom_list = df_unknown['u'].map(get_domain).tolist()\n",
        "    U_buc_list = df_unknown['u'].map(bucket_id).tolist()\n",
        "\n",
        "    # Map domains/buckets to node ids\n",
        "    dom2id, buc2id = {}, {}\n",
        "    def get_id(m, key, base):\n",
        "        if key not in m: m[key] = base + len(m)\n",
        "        return m[key]\n",
        "    nid = nu\n",
        "    dom_ids = [get_id(dom2id, d, nid) for d in U_dom_list]\n",
        "    nid = nu + len(dom2id)\n",
        "    buc_ids = [get_id(buc2id, b, nid) for b in U_buc_list]\n",
        "    N = nu + len(dom2id) + len(buc2id)\n",
        "\n",
        "    # Priors\n",
        "    prior = np.zeros((N,2), float)\n",
        "    prior[:nu,:] = P_unknown\n",
        "\n",
        "    # Aggregate observed labels to domains\n",
        "    if len(df_observed):\n",
        "        obs_dom = df_observed['u'].map(get_domain)\n",
        "        y_obs = ensure_labels(df_observed['label'])\n",
        "        dom_counts = defaultdict(lambda:[0,0])\n",
        "        for d, y in zip(obs_dom, y_obs):\n",
        "            if d in dom2id:\n",
        "                dom_counts[d][y] += 1\n",
        "        for d, did in dom2id.items():\n",
        "            g,b = dom_counts[d][0], dom_counts[d][1]\n",
        "            s = g+b\n",
        "            if s>0:\n",
        "                p1 = b/(s+1e-9); prior[did,:] = [1-p1, p1]\n",
        "            else:\n",
        "                prior[did,:] = [0.5,0.5]\n",
        "    else:\n",
        "        for d, did in dom2id.items():\n",
        "            prior[did,:] = [0.5,0.5]\n",
        "\n",
        "    for b, bid in buc2id.items():\n",
        "        prior[bid,:] = [0.5,0.5]\n",
        "\n",
        "    # Edges\n",
        "    edges = []\n",
        "    for i,(di,bi) in enumerate(zip(dom_ids, buc_ids)):\n",
        "        edges.append((i, di)); edges.append((i, bi))\n",
        "\n",
        "    # Similarity cache\n",
        "    sim_cache = { (i, di): 0.8 for i,di in enumerate(dom_ids) }\n",
        "    sim_cache.update({ (i, bi): 1.0 for i,bi in enumerate(buc_ids) })\n",
        "\n",
        "    # Neighbor list\n",
        "    neighbors = [[] for _ in range(N)]\n",
        "    for (u,v) in list(edges)+[(v,u) for (u,v) in edges]:\n",
        "        neighbors[u].append(v)\n",
        "\n",
        "    return prior, neighbors, edges, sim_cache, {\"nu\":nu}\n",
        "\n",
        "def lbp_infer(prior, neighbors, edges, sim_cache, iters=6, tau=0.5):\n",
        "    # min-sum BP\n",
        "    msg = { (u,v): np.zeros(2) for (u,v) in edges }\n",
        "    rev = { (v,u): np.zeros(2) for (u,v) in edges }\n",
        "    msg.update(rev)\n",
        "\n",
        "    def psi(sim, same, ths_plus=CONFIG['ths_plus'], ths_minus=CONFIG['ths_minus']):\n",
        "        return max(1e-6, min(ths_plus, 1.0 - sim)) if same else max(ths_minus, sim)\n",
        "\n",
        "    for _ in range(iters):\n",
        "        new_msg = {}\n",
        "        for u, vs in enumerate(neighbors):\n",
        "            for v in vs:\n",
        "                msum = np.zeros(2)\n",
        "                for w in neighbors[u]:\n",
        "                    if w==v: continue\n",
        "                    msum += msg[(w,u)]\n",
        "                out = np.zeros(2)\n",
        "                sim = sim_cache.get((u,v), 0.5)\n",
        "                for l in (0,1):\n",
        "                    cands = []\n",
        "                    for lp in (0,1):\n",
        "                        cands.append((1 - prior[u,lp]) + psi(sim, l==lp) + msum[lp])\n",
        "                    out[l] = min(cands)\n",
        "                new_msg[(u,v)] = out\n",
        "        msg.update(new_msg)\n",
        "\n",
        "    # Collect decisions for first nu nodes (unknown URLs)\n",
        "    nu = np.where(np.all(prior==prior, axis=1))[0].shape[0]  # not used; will compute from neighbors\n",
        "    # better: assume unknown URLs are those with any neighbor\n",
        "    nu = 0\n",
        "    for u in range(len(neighbors)):\n",
        "        # unknown URL nodes have been placed at the beginning in build_graph_for\n",
        "        if u==0 or u<nu: pass\n",
        "    # we pass nu via idx_maps\n",
        "    return msg\n"
      ],
      "id": "PhRK79H73yxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9aFe4rQ3yxv"
      },
      "source": [
        "# ==========================\n",
        "# Validation routine for a given Stage A model: run τ-sweep on VALID\n",
        "# ==========================\n",
        "def validate_tau(vec, clf, cal, train_rest_df, valid_df):\n",
        "    # Stage A on VALID\n",
        "    P_valid, t_pred = predict_proba_stageA(vec, clf, cal, valid_df)\n",
        "    y_valid = ensure_labels(valid_df['label'])\n",
        "    low, high = CONFIG['uncertain_low'], CONFIG['uncertain_high']\n",
        "    mask = (P_valid[:,1]>=low) & (P_valid[:,1]<=high)\n",
        "    idx = np.where(mask)[0]\n",
        "    print(f\"VALID uncertain in window [{low:.2f},{high:.2f}]: {len(idx)} of {len(valid_df)}\")\n",
        "\n",
        "    # Build graph using: unknown=valid[mask], observed=train_rest\n",
        "    prior, neighbors, edges, sim_cache, meta = build_graph_for(valid_df.iloc[idx], train_rest_df, P_valid[idx])\n",
        "\n",
        "    # Run LBP once and cache costs; then sweep tau cheaply\n",
        "    # Run inference to get messages\n",
        "    msg = { (u,v): np.zeros(2) for (u,v) in edges }\n",
        "    rev = { (v,u): np.zeros(2) for (u,v) in edges }\n",
        "    msg.update(rev)\n",
        "\n",
        "    def psi(sim, same, ths_plus=CONFIG['ths_plus'], ths_minus=CONFIG['ths_minus']):\n",
        "        return max(1e-6, min(ths_plus, 1.0 - sim)) if same else max(ths_minus, sim)\n",
        "\n",
        "    for _ in range(CONFIG['lbp_iters']):\n",
        "        new_msg = {}\n",
        "        for u, vs in enumerate(neighbors):\n",
        "            for v in vs:\n",
        "                msum = np.zeros(2)\n",
        "                for w in neighbors[u]:\n",
        "                    if w==v: continue\n",
        "                    msum += msg[(w,u)]\n",
        "                out = np.zeros(2)\n",
        "                sim = sim_cache.get((u,v), 0.5)\n",
        "                for l in (0,1):\n",
        "                    cands = []\n",
        "                    for lp in (0,1):\n",
        "                        cands.append((1 - prior[u,lp]) + psi(sim, l==lp) + msum[lp])\n",
        "                    out[l] = min(cands)\n",
        "                new_msg[(u,v)] = out\n",
        "        msg.update(new_msg)\n",
        "\n",
        "    # Compute URL costs once\n",
        "    nu = len(valid_df.iloc[idx])\n",
        "    costs = np.zeros((nu,2))\n",
        "    for i in range(nu):\n",
        "        msum = np.zeros(2)\n",
        "        for v in neighbors[i]:\n",
        "            msum += msg[(v,i)]\n",
        "        costs[i,0] = (1 - prior[i,0]) + msum[0]\n",
        "        costs[i,1] = (1 - prior[i,1]) + msum[1]\n",
        "\n",
        "    # Sweep tau\n",
        "    best = {\"tau\": 0.5, \"f1\": -1}\n",
        "    for tau in (CONFIG['tau_grid'] if CONFIG['enable_tau_sweep'] else [0.5]):\n",
        "        ratio = costs[:,1] / (costs[:,0] + 1e-9)\n",
        "        yU = (ratio < tau).astype(int)\n",
        "        y_pred = (P_valid[:,1] >= 0.5).astype(int)\n",
        "        y_pred[idx] = yU\n",
        "        _,_,_,f1 = metrics(y_valid, y_pred)\n",
        "        if f1 > best['f1']:\n",
        "            best = {\"tau\": tau, \"f1\": f1}\n",
        "    print(f\"Best τ on VALID: {best['tau']:.2f} (F1={best['f1']:.4f}) | Build {t_build:.2f}s, LBP {t_lbp:.2f}s\")\n",
        "    return best['tau']"
      ],
      "id": "Z9aFe4rQ3yxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id8XfZeW3yxv"
      },
      "source": [
        "# ==========================\n",
        "# Class-weight sweep (optional) — returns best (vec,clf,cal) and timings\n",
        "# ==========================\n",
        "def sweep_class_weight(train_rest_df, y_rest, valid_df):\n",
        "    best = {\"f1\": -1}\n",
        "    for w in CONFIG['pos_weight_grid']:\n",
        "        print(f\"\\n>>> Trying pos_class_weight={w}\")\n",
        "        vec, clf, cal, tim = train_stageA(train_rest_df, y_rest, pos_weight=w)\n",
        "        # quick stage-A on VALID for sanity\n",
        "        P_valid, _ = predict_proba_stageA(vec, clf, cal, valid_df)\n",
        "        y_valid = ensure_labels(valid_df['label'])\n",
        "        y_pred_A = (P_valid[:,1]>=0.5).astype(int)\n",
        "        _,_,_,f1A = metrics(y_valid, y_pred_A)\n",
        "        print(f\"Stage-A VALID F1={f1A:.4f}\")\n",
        "        # tau validation via LBP\n",
        "        tau = validate_tau(vec, clf, cal, train_rest_df, valid_df)\n",
        "        # measure final F1 on VALID with that tau (already printed during validate_tau)\n",
        "        if f1A > best['f1']:  # or keep the tau-based f1; here we keep the LBP-picked tau value\n",
        "            best = {\"w\": w, \"vec\": vec, \"clf\": clf, \"cal\": cal, \"tim\": tim, \"tau\": tau, \"f1\": f1A}\n",
        "    print(f\"\\nBest class weight={best['w']} with VALID Stage-A F1={best['f1']:.4f}; τ={best['tau']:.2f}\")\n",
        "    return best['vec'], best['clf'], best['cal'], best['tim'], best['tau']"
      ],
      "id": "Id8XfZeW3yxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37FCGc6C3yxv",
        "outputId": "7ed9fc64-8f02-4469-c1df-1bac8765c302"
      },
      "source": [
        "# ==========================\n",
        "# Train Stage A (with optional class-weight sweep) and pick τ on VALID\n",
        "# ==========================\n",
        "if CONFIG['enable_class_weight_sweep']:\n",
        "    vec, clf, cal, tim_train, tau_best = sweep_class_weight(train_rest, y_rest, valid)\n",
        "else:\n",
        "    # Single training using 'balanced' to respect class imbalance, faster\n",
        "    print(\"Training Stage A with class_weight='balanced' (fast path)...\")\n",
        "    # Temporarily call train_stageA with a sentinel to instruct 'balanced'\n",
        "    vec, clf, cal, tim_train = train_stageA(train_rest, y_rest, pos_weight='balanced')\n",
        "    tau_best = validate_tau(vec, clf, cal, train_rest, valid)\n",
        "\n",
        "print(\"\\nStage A timing (train path):\")\n",
        "for k,v in tim_train.items():\n",
        "    print(f\"  {k:10s}: {v:.2f}s\")\n",
        "print(f\"Chosen τ (from VALID): {tau_best}\")"
      ],
      "id": "37FCGc6C3yxv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stage A with class_weight='balanced' (fast path)...\n",
            "VALID uncertain in window [0.40,0.60]: 193 of 60000\n",
            "Best τ on VALID: 0.45 (F1=0.6421) | Build 1.30s, LBP 0.03s\n",
            "\n",
            "Stage A timing (train path):\n",
            "  vectorize : 50.93s\n",
            "  train     : 6.25s\n",
            "  calibrate : 0.09s\n",
            "Chosen τ (from VALID): 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zKjVoPG3yxw",
        "outputId": "6e25f8ca-f777-4e3e-9788-fa8b85aed72e"
      },
      "source": [
        "# ==========================\n",
        "# Final EVALUATION on TEST (with full timing breakdown)\n",
        "# ==========================\n",
        "print(\"\\n=== Testing on TEST set ===\")\n",
        "P_test, t_predA = predict_proba_stageA(vec, clf, cal, test)\n",
        "y_pred = (P_test[:,1]>=0.5).astype(int)\n",
        "\n",
        "# Uncertain slice\n",
        "low, high = CONFIG['uncertain_low'], CONFIG['uncertain_high']\n",
        "mask = (P_test[:,1]>=low) & (P_test[:,1]<=high)\n",
        "idx = np.where(mask)[0]\n",
        "print(f\"TEST uncertain in window [{low:.2f},{high:.2f}]: {len(idx)} of {len(test)}\")\n",
        "\n",
        "# Build graph for TEST: unknown = test[idx], observed = train_rest\n",
        "prior, neighbors, edges, sim_cache, meta = build_graph_for(test.iloc[idx], train_rest, P_test[idx])\n",
        "\n",
        "# run min-sum\n",
        "msg = { (u,v): np.zeros(2) for (u,v) in edges }\n",
        "rev = { (v,u): np.zeros(2) for (u,v) in edges }\n",
        "msg.update(rev)\n",
        "def psi(sim, same, ths_plus=CONFIG['ths_plus'], ths_minus=CONFIG['ths_minus']):\n",
        "    return max(1e-6, min(ths_plus, 1.0 - sim)) if same else max(ths_minus, sim)\n",
        "for _ in range(CONFIG['lbp_iters']):\n",
        "    new_msg = {}\n",
        "    for u, vs in enumerate(neighbors):\n",
        "        for v in vs:\n",
        "            msum = np.zeros(2)\n",
        "            for w in neighbors[u]:\n",
        "                if w==v: continue\n",
        "                msum += msg[(w,u)]\n",
        "            out = np.zeros(2)\n",
        "            sim = sim_cache.get((u,v), 0.5)\n",
        "            for l in (0,1):\n",
        "                cands = []\n",
        "                for lp in (0,1):\n",
        "                    cands.append((1 - prior[u,lp]) + psi(sim, l==lp) + msum[lp])\n",
        "                out[l] = min(cands)\n",
        "            new_msg[(u,v)] = out\n",
        "    msg.update(new_msg)\n",
        "\n",
        "# Decide labels for unknown URLs using chosen τ\n",
        "nu = len(test.iloc[idx])\n",
        "costs = np.zeros((nu,2))\n",
        "for i in range(nu):\n",
        "    msum = np.zeros(2)\n",
        "    for v in neighbors[i]:\n",
        "        msum += msg[(v,i)]\n",
        "    costs[i,0] = (1 - prior[i,0]) + msum[0]\n",
        "    costs[i,1] = (1 - prior[i,1]) + msum[1]\n",
        "ratio = costs[:,1] / (costs[:,0] + 1e-9)\n",
        "yU = (ratio < tau_best).astype(int)\n",
        "\n",
        "y_pred[idx] = yU\n",
        "\n",
        "acc,prec,rec,f1 = metrics(y_test, y_pred)\n",
        "print(f\"\\nFINAL METRICS (TEST): Acc={acc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\")"
      ],
      "id": "9zKjVoPG3yxw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing on TEST set ===\n",
            "TEST uncertain in window [0.40,0.60]: 1045 of 361934\n",
            "\n",
            "FINAL METRICS (TEST): Acc=0.9875  Prec=0.9217  Rec=0.4777  F1=0.6292\n",
            "\n",
            "TIMINGS (TEST path)\n",
            "----------------------------------------\n",
            "Stage A predict      : 15.37s\n",
            "Graph build (LBP)    : 1.30s\n",
            "LBP inference        : 0.22s\n",
            "Stitch predictions   : 0.00s\n",
            "----------------------------------------\n",
            "End-to-end (TEST)    : 16.94s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}